{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandretorres/miniconda3/envs/torch23/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from jamba import JambaLMConfig as myJambaLMConfig, JambaLM as myJambaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. If you want to use the naive implementation, set `use_mamba_kernels=False` in the model config\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"TechxGenus/Mini-Jamba\", trust_remote_code=True,)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TechxGenus/Mini-Jamba\", torch_dtype=torch.float16, use_mamba_kernels=False, \n",
    "                                             device_map=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A Python function isipv chambre Inn Iterate\\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n        \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"A Python function is\"\n",
    "\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs, max_new_tokens=64, do_sample=False)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_jamba = myJambaLMConfig(vocab_size=model.config.vocab_size, d_model=model.config.hidden_size, n_layers=model.config.num_hidden_layers, \n",
    "                               rms_norm_eps=model.config.rms_norm_eps, mlp_size=model.config.intermediate_size, inner_layernorms=model.config.mamba_inner_layernorms,\n",
    "                               expand_factor=model.config.mamba_expand, dt_rank=model.config.mamba_dt_rank, d_state=model.config.mamba_d_state,\n",
    "                               d_conv=model.config.mamba_d_conv, conv_bias=model.config.mamba_conv_bias, initializer_range=model.config.initializer_range,\n",
    "                               num_experts=model.config.num_experts, num_experts_per_tok=model.config.num_experts_per_tok, \n",
    "                               attn_layer_offset=model.config.attn_layer_offset, attn_layer_period=model.config.attn_layer_period, \n",
    "                               expert_layer_offset=model.config.expert_layer_offset, expert_layer_period=model.config.expert_layer_period,\n",
    "                               num_key_value_heads=model.config.num_key_value_heads, num_attention_heads=model.config.num_attention_heads,\n",
    "                               pad_token_id=model.config.pad_token_id, bias=model.config.mamba_proj_bias, attention_dropout=model.config.attention_dropout,\n",
    "                               tie_lm_weights=model.config.tie_word_embeddings)\n",
    "\n",
    "my_model = myJambaLM(config_jamba)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    name = name.replace(\"model.\", \"jamba.\")\n",
    "    \n",
    "    if \"embed_tokens\" in name:\n",
    "        name = \"embedding.weight\"\n",
    "    \n",
    "    if \"final_layernorm\" in name:\n",
    "        name = name.replace(\"jamba.\", \"\")\n",
    "\n",
    "    counterpart_param = my_model.get_parameter(name)\n",
    "    if counterpart_param is not None:\n",
    "        counterpart_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=60, size=(10, 12))\n",
    "torch.allclose(model(x).logits, my_model(x), atol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-19.3125, -19.3750,  26.2031,  ..., -19.3594, -19.4219, -19.2188],\n",
       "         [-22.5312, -22.5156,  27.9219,  ..., -22.6094, -22.5781, -22.3594],\n",
       "         [-21.9219, -21.8594,  28.4531,  ..., -21.9531, -21.9531, -21.7500],\n",
       "         ...,\n",
       "         [-26.5000, -26.4688,  30.5312,  ..., -26.6406, -26.5156, -26.4219],\n",
       "         [-28.5625, -28.4844,  31.1250,  ..., -28.6875, -28.5312, -28.4844],\n",
       "         [-28.9688, -28.8594,  31.4375,  ..., -29.1094, -28.9531, -28.8438]],\n",
       "\n",
       "        [[-25.2656, -25.1719,  28.7344,  ..., -25.3125, -25.2656, -25.2031],\n",
       "         [-27.7969, -27.6719,  30.7969,  ..., -27.9219, -27.8125, -27.7656],\n",
       "         [-26.9375, -26.8438,  30.3750,  ..., -27.0938, -26.9531, -26.9062],\n",
       "         ...,\n",
       "         [-27.8125, -27.7031,  32.2812,  ..., -27.9219, -27.7969, -27.7188],\n",
       "         [-28.0000, -27.9062,  32.3125,  ..., -28.0625, -27.9688, -27.9062],\n",
       "         [-27.6562, -27.5625,  32.2500,  ..., -27.7031, -27.6250, -27.5781]],\n",
       "\n",
       "        [[-22.2656, -22.2500,  26.8125,  ..., -22.3906, -22.3125, -22.2344],\n",
       "         [-23.7344, -23.6719,  26.9844,  ..., -23.9219, -23.7812, -23.7188],\n",
       "         [-27.6719, -27.6250,  30.2031,  ..., -27.8438, -27.7344, -27.6562],\n",
       "         ...,\n",
       "         [-29.0156, -28.9531,  32.6250,  ..., -29.1406, -29.0312, -28.9844],\n",
       "         [-27.5469, -27.4688,  32.0625,  ..., -27.6719, -27.5625, -27.5156],\n",
       "         [-27.2969, -27.2188,  32.2188,  ..., -27.3906, -27.2969, -27.2344]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-20.2031, -20.2188,  26.4531,  ..., -20.4375, -20.2031, -20.0000],\n",
       "         [-25.9062, -25.8594,  29.5938,  ..., -26.1250, -25.8594, -25.7969],\n",
       "         [-27.6719, -27.5312,  30.5469,  ..., -27.7656, -27.6562, -27.5000],\n",
       "         ...,\n",
       "         [-28.0938, -27.9531,  31.6719,  ..., -28.0938, -28.0469, -27.8750],\n",
       "         [-30.2500, -30.0781,  32.5938,  ..., -30.2812, -30.1719, -30.0625],\n",
       "         [-28.9062, -28.7500,  32.1250,  ..., -28.9062, -28.8125, -28.7031]],\n",
       "\n",
       "        [[-17.7344, -17.8906,   8.2969,  ..., -17.9531, -17.9844, -18.0469],\n",
       "         [-16.0781, -16.3281,  17.4375,  ..., -16.2344, -16.2812, -16.3125],\n",
       "         [-18.9688, -19.1250,  21.0156,  ..., -19.0000, -19.0938, -18.9844],\n",
       "         ...,\n",
       "         [-25.4688, -25.5312,  28.8750,  ..., -25.5000, -25.5156, -25.2969],\n",
       "         [-26.4062, -26.4531,  29.5938,  ..., -26.4531, -26.4375, -26.3281],\n",
       "         [-24.7969, -24.8438,  29.7656,  ..., -24.8906, -24.8281, -24.6719]],\n",
       "\n",
       "        [[-23.2969, -23.3438,  25.4844,  ..., -23.5000, -23.4844, -23.3438],\n",
       "         [-25.7969, -25.7500,  28.7031,  ..., -25.9844, -25.9219, -25.8125],\n",
       "         [-27.2344, -27.1406,  29.9375,  ..., -27.3125, -27.3125, -27.1719],\n",
       "         ...,\n",
       "         [-27.2500, -27.1406,  30.9062,  ..., -27.2969, -27.1719, -27.0781],\n",
       "         [-26.2969, -26.2188,  31.2188,  ..., -26.3125, -26.2656, -26.1250],\n",
       "         [-26.5469, -26.4844,  31.4375,  ..., -26.5781, -26.5156, -26.3750]]],\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-19.3141, -19.3836,  26.2029,  ..., -19.3653, -19.4294, -19.2251],\n",
       "         [-22.5247, -22.5133,  27.9237,  ..., -22.6124, -22.5718, -22.3550],\n",
       "         [-21.9077, -21.8545,  28.4431,  ..., -21.9459, -21.9527, -21.7437],\n",
       "         ...,\n",
       "         [-26.4991, -26.4622,  30.5308,  ..., -26.6301, -26.5195, -26.4149],\n",
       "         [-28.5515, -28.4696,  31.1276,  ..., -28.6707, -28.5148, -28.4668],\n",
       "         [-28.9560, -28.8569,  31.4315,  ..., -29.0955, -28.9422, -28.8281]],\n",
       "\n",
       "        [[-25.2732, -25.1907,  28.7347,  ..., -25.3240, -25.2770, -25.2212],\n",
       "         [-27.7995, -27.6675,  30.7923,  ..., -27.9272, -27.8126, -27.7696],\n",
       "         [-26.9448, -26.8476,  30.3769,  ..., -27.0928, -26.9667, -26.9075],\n",
       "         ...,\n",
       "         [-27.8120, -27.7041,  32.2676,  ..., -27.9230, -27.7895, -27.7203],\n",
       "         [-28.0004, -27.9071,  32.3101,  ..., -28.0653, -27.9692, -27.9188],\n",
       "         [-27.6509, -27.5559,  32.2521,  ..., -27.7057, -27.6208, -27.5710]],\n",
       "\n",
       "        [[-22.2703, -22.2582,  26.8164,  ..., -22.4047, -22.3162, -22.2459],\n",
       "         [-23.7366, -23.6825,  26.9827,  ..., -23.9345, -23.7895, -23.7303],\n",
       "         [-27.6870, -27.6287,  30.2029,  ..., -27.8490, -27.7422, -27.6608],\n",
       "         ...,\n",
       "         [-29.0251, -28.9485,  32.6301,  ..., -29.1500, -29.0371, -28.9830],\n",
       "         [-27.5561, -27.4820,  32.0787,  ..., -27.6810, -27.5750, -27.5292],\n",
       "         [-27.2949, -27.2256,  32.2356,  ..., -27.3927, -27.3037, -27.2336]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-20.2046, -20.2114,  26.4625,  ..., -20.4286, -20.1961, -20.0060],\n",
       "         [-25.9170, -25.8570,  29.5988,  ..., -26.1290, -25.8654, -25.7988],\n",
       "         [-27.6786, -27.5271,  30.5572,  ..., -27.7614, -27.6626, -27.5070],\n",
       "         ...,\n",
       "         [-28.0871, -27.9400,  31.6672,  ..., -28.0938, -28.0413, -27.8624],\n",
       "         [-30.2476, -30.0706,  32.6000,  ..., -30.2666, -30.1651, -30.0558],\n",
       "         [-28.9006, -28.7487,  32.1229,  ..., -28.8959, -28.8107, -28.7007]],\n",
       "\n",
       "        [[-17.7528, -17.8940,   8.3092,  ..., -17.9635, -17.9897, -18.0613],\n",
       "         [-16.0806, -16.3407,  17.4477,  ..., -16.2379, -16.2912, -16.3272],\n",
       "         [-18.9748, -19.1216,  21.0258,  ..., -18.9999, -19.1002, -18.9903],\n",
       "         ...,\n",
       "         [-25.4729, -25.5422,  28.8800,  ..., -25.5083, -25.5210, -25.3104],\n",
       "         [-26.4102, -26.4490,  29.5917,  ..., -26.4515, -26.4407, -26.3243],\n",
       "         [-24.8014, -24.8484,  29.7659,  ..., -24.9029, -24.8388, -24.6794]],\n",
       "\n",
       "        [[-23.3030, -23.3541,  25.4901,  ..., -23.4999, -23.4912, -23.3536],\n",
       "         [-25.7882, -25.7433,  28.7082,  ..., -25.9784, -25.9178, -25.8039],\n",
       "         [-27.2390, -27.1412,  29.9374,  ..., -27.3092, -27.3110, -27.1734],\n",
       "         ...,\n",
       "         [-27.2538, -27.1335,  30.8983,  ..., -27.2965, -27.1739, -27.0757],\n",
       "         [-26.2956, -26.2143,  31.2263,  ..., -26.3130, -26.2616, -26.1324],\n",
       "         [-26.5449, -26.4811,  31.4424,  ..., -26.5817, -26.5078, -26.3664]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
