{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the PyTorch parallel scan, with RNNs/Mamba in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file gives a detailed background and explanation of the `pscan.py` file. The goal is to impement a parallel scan in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, what is a <b>scan</b> ?\n",
    "\n",
    "A scan is defined as an operation that takes as input an array and procudes an array as output. You can see that it is quite general.\n",
    "\n",
    " A simple and well-known example of a scan is the <i>cumulative sum of an array</i> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  3,  6, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([1, 2, 3, 4])\n",
    "\n",
    "torch.cumsum(X, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this document, we will denote `L` as the length of our input array `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic way to implement a scan is to use a simple for loop :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  3,  6, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.zeros_like(X)\n",
    "\n",
    "cumulative_sum = 0\n",
    "for t in range(X.size(0)):\n",
    "    cumulative_sum += X[t]\n",
    "    Y[t] = cumulative_sum\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite simple for now, right ?\n",
    "To setup our notations, we will keep this example for a bit.\n",
    "\n",
    "Here, we use an accumulator, `cumulative_sum`, which we update as we go through the input array `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An equivalent way to rewrite the above code is :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  3,  6, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.zeros_like(X)\n",
    "\n",
    "Y[0] = X[0]\n",
    "for t in range(1, X.size(0)):\n",
    "    Y[t] = Y[t-1] + X[t]\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not explicitly present, the accumulator is still here, in `Y`. It is propagated with the recurrence relation `Y[t] = Y[t-1] + X[t]`.\n",
    "\n",
    "We can visualize what happens with a simple diagram, which should remind you a bit about RNNs :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"assets/cumsum_rnns.jpg\" alt=\"cumulative sum\" width=\"1000\" height=\"300\" alt=\"python mamba\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some sense, `Y` plays the role of the hidden state, while `X` plays the role of the input : as we process the input, we keep and update a running hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this method of computation, which uses a sequential loop, induces `L` sequential steps of computations in order to compute the whole output `Y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, is it possible to <b>parallelize</b> this scan operation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just what does the <b>parallel scan</b>. \n",
    "\n",
    "Let's stay with the simple example of our cumulative sum. In fact, let's simplify it even more : let's say with just want to compute the sum of our input array `X`.\n",
    "Again, we could come up with a for loop to count the elements. But can't we parallelize this computation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, and it is best visualized with this simple tree :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"assets/reduction_tree.jpg\" alt=\"cumulative sum\" width=\"1000\" height=\"600\" alt=\"python mamba\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that the length of our array is a power of 2, then we just have to group the elements 2 by 2, add them, and repeat until we are left with one element, our result. If `L` is `2**d`, then we will need to do `math.log2(L)` sequential steps to compute the sum of the array. That's a major speedup over the `L` steps of the naive for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we implement this in Python ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]) #Â input array\n",
    "L = X.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can <i>group the elements by two</i> using :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6],\n",
       "        [7, 8]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa = X.view(L//2, 2)\n",
    "Xa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have pairs of elements. We can do :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6, 8])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to access the elements from the two groups. We can see that, to compute the first step, we simply need to sum these two arrays :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  7, 11, 15])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa[:, 0] + Xa[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay ! We have just accomplished our first step ! Now, we just need to repeat what we've just done.\n",
    "\n",
    "Note that we could do `Xa = Xa[:, 0] + Xa[:, 1]` and then we would just need to repeat the previous step. But :\n",
    "- this will allocate extra memory spaces for storing the result of the first step (`Xa` currently shares the data as `X`).\n",
    "- we will reuse some of these values, later, for the full scan operation.\n",
    "\n",
    "Hence, we will work by updating `X` <b>in-place</b>, by doing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  7, 11, 15])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa[:, 1] += Xa[:, 0]\n",
    "Xa[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we want. Now, we will repeat the step we have just done, but on `Xa[:, 1]` rather than on `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa = Xa[:, 1]\n",
    "Xa = Xa.view(Xa.size(0)//2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, `Xa` is split in two groups :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  7],\n",
       "        [11, 15]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sum these two groups, and put the result in the second half of `Xa`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 26])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa[:, 1] += Xa[:, 0]\n",
    "Xa[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two elements left ! This means, only one more step to go (because `math.log2(2) = 1` of course !) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa = Xa[:, 1]\n",
    "Xa = Xa.view(Xa.size(0)//2, 2)\n",
    "\n",
    "Xa[:, 1] += Xa[:, 0]\n",
    "Xa[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the result we want !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
